Title: Writeup for Project 4, Fall 2015
Date: 12/1/15
Group:  Jackson Brodeur     jbrodeur@usc.edu
        Dylan Davis         dylandav@usc.edu
        Matthew O'Brien     obrienm@usc.edu
		Yekaterina Glazko   glazko@usc.edu
 
I. Requirements:
	Part 1:
		We must write a user program for each unique entity in the passport office such as customer, application clerk, picture clerk, etc… These user programs will all run as their own process since we Exec all of them (and not fork). In this part, we will use only 1 server to connect all of the entities that we have Execed. The server will connect the various entities together and transfer data between user programs. For example, when a customer approaches a picture clerk, the server will transfer ID numbers and other data between these two user programs rather than have the user programs talk to each other directly. Also, the remote procedure calls (RPCs) that allow the passport to work properly and prevent race conditions are not visible to the user programs. To a client user program or a clerk user program (or any other entity for that matter) the Acquire, Wait, and various other RPCs that are called are seen as simple system calls on the client level.

	Part 2:
		In this part, we must create a distributed server system rather than having only 1 server as we did in Part 1. We may have up to 5 servers in this part and the user programs will pick a random server to communicate with when they need to transfer data. Having multiple servers means that we must change our algorithms in regards to how locks, CVs, and MVs are created and accessed. Since each server has only the variables that they created, we must now make each servers communicate with each other. If a server does not have a requested variable, they must message all other servers to see if anyone else has it before creating the requested variable (in Part 1 if the lone server didn’t have a variable then we could safely assume that it did not exist). Simply put, we must create multiple servers and make all of our servers communicate with each other in order to keep track of the various shared variables created by the passport office.

II. Assumptions:
	Our main assumption is that our RPCs will all work as aspected regardless of the fact that they are now RPCs instead of system calls. In other words, any time a user program needs to access shared state such as locks/CVs/MVs they will be able to call the standard system call (Acquire, Signal, Wait, Get, etc..) and they will receive the correct output regardless of the fact that we changed the system calls and shared state to be handled on multiple servers now. This means we should not have to change any logic in our passport office, we must merely change any direct access of variables to the proper RPC.
	We also assumed that the servers would always be consistent in their responses and never drop a request or a response. Thus, if a check was sent by one server to see if any other server had a requested variable, all of the servers would respond eventually.

III. Design:
	We decided to implement only 1 set of MV calls with all MVs as an array and would thus need to send the index within the given MV. Even MVs that are only 1 int, such as customersFinished (which keeps track of the total number of customers who have had their passports completed) are stored as an array. To access the variable in that case, you would always pass index 0 as the value of the array requested. For variables used by clerks, like lineCount, you would pass the proper index of the MV (in this case the index at the clerks line number).
	Algorithms for our passport office will not change because we assume our RPCs to be correct and return the same values as the system calls we used for previous projects - the interface has remained the same despite the backend being modified. This is explained in the previous “assumptions” section.
	We decided to keep the servers single threaded and have one combined inbox of messages from clients as well as other servers. We created a Request struct to handle each request for a resource and included it's RPC type, name, a counter for "No"s received, client machine ID and mailbox as well as other pertinent information. To keep track of all these requests we used a map that mapped a request ID to each Request struct. To keep track of the server and client messages we used the following notation: syscalls were prepended with an "S" if coming from a server (i.e. ACQUIRE - message from a client & S_ACQUIRE - message from another server). For the creates we used a function that would check to see if we had a pending request for the same resource when being queried for any given resource. On a create we would first check to see if we had it but this time we would send a message to all other servers if we didn't have it. If another server did have the resource then it would take over the rest of the process and return a YES to the original server. If it didn't it would return a NO to the original server. The original server would keep track of NO's and if all NO's were returned then it would finally create the given resource. With the other resource calls (acquire, release, wait, signal, broadcast, get, set) a similar process is followed with the exception that it would be an error if all servers reply NO and no resource would be created. The same part is that the server with the resource will take over the job and reply YES to the original server. With the CV syscalls it became a two step process because it needed both a lock and a CV. So there were four cases - a server had: both resources, only the lock, only the CV, and neither. 


	

IV. Implementation:
	+ Files Modified
		In the test directory:
			Makefile

		In the userprog directory:
			syscall.h
			exception.cc
			addrspace.cc
			addrspace.h
			progtest.cc

		In the network directory:
			nettest.cc
			Makefile

		In the machine directory:
			machine.h
			translate.h

		In the threads directory:
			system.h
			system.cc
			main.cc
		
	+ Files added
		In the test directory:
			clerks1.c
			clerks2.c
			customers1.c
			customers2.c
			senators.c
			managers.c
			setup.h
			passportclerk.c
			manager.c
			cashier.c
			pictureclerk.c
			applicationclerk.c

		
	+ Data Structures added, and the file they were added to.
		struct Request {     -- in nettest.cc
		    int rpcType;
		    char * name;
		    int index;
		    int lockIndex;
		    int conditionIndex;
		    int value;
		    int noCounter;
		    int fromMachineID;
		    int fromMailbox;
		}

		map<int, Request*> requestTable;   -- in nettest.cc


	+ Data Structures modified, and the file they were added to.
			class Thread		-- in file threads.h
			{
				+  int myMailbox;
			}

	+ Functions added and in which file.
		bool haveSameCreate(char* name); -- in file nettest.cc
		void TakeAction(int requestID); -- in file nettest.cc

	+ Functions modified and in which file.
		-- in file exception.cc
		int ExecSyscall(int vaddr, int len);
		void WaitSyscall(int conditionIndex, int lockIndex);

		-- in file nettest.cc
		int FindLock(char* name);
		int CreateLock(char* name);
		bool AcquireLock(int index, int machineID, int mailbox);
		bool ReleaseLock(int index, int machineID, int mailbox);

		int FindCV(char* name);
		int CreateCV(char* name);
		bool Wait(int conditionIndex, int lockIndex, int machineID, int mailbox);
		void Signal(int conditionIndex, int lockIndex);
		void Broadcast(int conditionIndex, int lockIndex);

		int FindMV(char* name);
		int CreateMV(char* name);
		void SetMV(int serverIndex, int mvIndex, int value);
		int GetMV(int serverIndex, int mvIndex);


V. Testing:  (For each test case, you must show)
+ How to test
	1) In order to test the Monitor Variable functionality with multiple servers, do the following:
		In terminal window 1 enter the command 'nachos -S -m 0'
		In terminal window 2 enter the command 'nachos -S -m 1'
		In terminal window 3 enter the command 'nachos -S -m 2'
		In terminal window 4 enter the command 'nachos -S -m 3'
		In terminal window 5 enter the command 'nachos -S -m 4'
		In terminal window 6 enter the command 'nachos -x ../test/multimv -m 10'
	2) You can test the Lock Syscalls together with the Condition Syscalls because the CV's rely on proper lock implementations
		In order to test Lock Create/Acquire/Release and CV Create/Wait/Signal do the following:
			In terminal window 1 enter the command 'nachos -S -m 0'
			In terminal window 2 enter the command 'nachos -S -m 1'
			In terminal window 3 enter the command 'nachos -S -m 2'
			In terminal window 4 enter the command 'nachos -S -m 3'
			In terminal window 5 enter the command 'nachos -S -m 4'
			In terminal window 6 enter the command 'nachos -x ../test/multiwait -m 10'
			In terminal window 7 enter the command 'nachos -x ../test/multisignal -m 11'
		A similar method with extra instances of 'nachos -x ../test/multiwait -m i' can be used to test broadcast with 'nachos -x ../test/mulitbroadcast -m 100'

	+ Test Output
		1) Monitor Variables
			In terminal window 6, the client, the output should be (note: x will vary based on which server the message is sent to):

				Creating mv at index: x
				Set mv x to 100
				Retrieved mv x with value 100
				Exiting with status 0

			In one of terminal windows 1-5 you should see the following output possibly with other debugging messages between them (note: x will vary based on the index of the mv) :

				Message from Machine [10:0]: 9 mv
				MV x[0] has been set to 100
				MV x[0] has been Retrieved with value 100

		2) Locks and Conditions
			 After running the client in terminal 6 the output should be (x is the lock index and y is the cv index):

			 	Acquired lock x
				Waiting on cv y with lock x

			After running the client in terminal 7 the output should be (x is the lock index and y is the cv index):

				Acquired lock x
				Signalled cv y with lock x
				Released lock x
				Exiting with status 0

			Ther output in terminal 6 should now be (x is the lock index and y is the cv index):
				Acquired lock 0
				Waiting on cv 400 with lock 0
				Released lock 0
				Exiting with status 0

			In the Server terminal windows you should see the following with other output possibly between them:

				Acquired lock x from machine [10:0]
				Released lock x from machine [10:0] <- happens when machine waits
				Retrieved lock named lock from machine 11
				Acquired lock x from machine [11:0]
				Adding [10:0] to waitQueue for lock x
				Lock 0 is busy so machine [10:0] will wait
				Machine [10:0] is now the owner of lock x
				Released lock x from machine [11:0]
				Released lock x from machine [10:0]

			When using the multibroadcast userprogram the output will be similar but there will be substantially more server output
	

VI. Discussion:
	+ Experiment expectation.  (What is supposed to happen.)
		Our expectation is that we will Exec any number (up to the max amount) of each instance in our passport office and the office will return the correct output and make sure all customers are finished. This of course will happen from the server communicating between all of the user programs together and keeping track of all the needed variables.

	+ Experiment result.  (What actually happened.)
		The result was that unfortunately we were not able to correctly implement the passport office as a set of distributed programs communicating with multiple servers. Instead we included a set of programs that can demonstrate the correctness of our RPCs across several servers.

VIII. Miscellaneous:
	Our passport office did not run correctly in the previous project and although we tried to convert it and fix it for this iteration we were unable to. It was only made harder due to the added difficulty of trying to debug across different machines. Therefore due to time constraints we settled on showcasing the multiple server RPCs using separate userprograms
